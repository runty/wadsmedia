---
phase: 19-webhook-server-resilience
plan: 02
type: execute
wave: 2
depends_on: ["19-01"]
files_modified:
  - src/plugins/health.ts
  - src/messaging/telegram-provider.ts
autonomous: true

must_haves:
  truths:
    - "Health endpoint returns structured status for webhook connectivity"
    - "Health endpoint returns structured status for LLM reachability"
    - "Health endpoint returns recent error rates"
    - "Overall status degrades to 'degraded' when any check fails"
  artifacts:
    - path: "src/plugins/health.ts"
      provides: "Structured health checks for database, webhook, LLM, and error rates"
      contains: "webhook"
    - path: "src/plugins/health.ts"
      provides: "Error rate tracking via in-memory counter"
      contains: "errorRate"
  key_links:
    - from: "src/plugins/health.ts"
      to: "src/messaging/telegram-provider.ts"
      via: "fastify.telegramMessaging.getWebhookInfo()"
      pattern: "telegramMessaging.*getWebhookInfo"
    - from: "src/plugins/health.ts"
      to: "fastify.llm"
      via: "OpenAI models.list() or similar lightweight call"
      pattern: "llm\\.models"
---

<objective>
Expand the health endpoint to return structured status for webhook connectivity, LLM reachability, and recent error rates.

Purpose: Operators need a single endpoint to assess system health at a glance. The current /health only checks database connectivity. After this plan, it will also report whether the Telegram webhook is properly registered, whether the LLM API is reachable, and the recent error rate from request processing.

Output: GET /health returns a comprehensive JSON response with check results for all subsystems and an aggregate status.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/19-webhook-server-resilience/19-01-SUMMARY.md
@src/plugins/health.ts
@src/plugins/telegram-messaging.ts
@src/messaging/telegram-provider.ts
@src/plugins/conversation.ts
@src/server.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add error rate tracking to the health plugin</name>
  <files>src/plugins/health.ts</files>
  <action>
Add an in-memory sliding-window error rate tracker to the health plugin. This tracks errors from the request lifecycle, not from specific subsystems.

Implementation:
1. Create a simple error counter at module scope:
   ```
   const errorTimestamps: number[] = [];
   const ERROR_WINDOW_MS = 5 * 60 * 1000; // 5-minute window
   ```

2. Register a Fastify `onResponse` hook that records errors:
   ```
   fastify.addHook('onResponse', (request, reply, done) => {
     if (reply.statusCode >= 500) {
       errorTimestamps.push(Date.now());
     }
     done();
   });
   ```

3. Create a helper function `getRecentErrorRate()` that:
   - Filters `errorTimestamps` to keep only entries within the last 5 minutes
   - Replaces the array contents with the filtered result (prunes old entries)
   - Returns `{ count: number, windowMinutes: 5 }`

4. Also export a `recordError()` function that manually pushes a timestamp. This is for cases where errors are caught and handled (like conversation processing failures) that don't result in 5xx responses. Wire this into the request lifecycle by adding an `onError` hook:
   ```
   fastify.addHook('onError', (request, reply, error, done) => {
     errorTimestamps.push(Date.now());
     done();
   });
   ```

This approach uses the same in-memory pattern as the group chat rate limiter (Map<string, number[]> with timestamp pruning), keeping the codebase consistent.
  </action>
  <verify>
Run `npx tsc --noEmit` to confirm type-checking. Read the file to confirm onResponse/onError hooks are registered and getRecentErrorRate function exists.
  </verify>
  <done>Error rate tracking is active via Fastify hooks, pruning old entries on each read, with a 5-minute sliding window.</done>
</task>

<task type="auto">
  <name>Task 2: Expand health endpoint with webhook, LLM, and error rate checks</name>
  <files>src/plugins/health.ts</files>
  <action>
Rewrite the GET /health handler to run multiple health checks in parallel and return a structured response.

The handler should:

1. **Database check** (existing): `fastify.db.run(sql\`SELECT 1\`)` -- same as current.

2. **Telegram webhook check** (new): If `fastify.telegramMessaging` exists, call `fastify.telegramMessaging.getWebhookInfo()` with a 5-second timeout (use AbortSignal.timeout(5000)).
   - If the returned `url` is non-empty and matches expected webhook URL (from `fastify.config.TELEGRAM_WEBHOOK_URL`), status is "ok".
   - If url is empty or doesn't match, status is "misconfigured" with detail.
   - If the call throws, status is "error" with error message.
   - If `fastify.telegramMessaging` is undefined, status is "not_configured".
   - Include `pending_update_count` and `last_error_message` in the response when available.

3. **LLM check** (new): If `fastify.llm` exists, call `fastify.llm.models.list()` with a 5-second timeout. This is a lightweight GET to /v1/models that confirms API key validity and service reachability.
   - If successful, status is "ok".
   - If it throws, status is "error" with the error message.
   - If `fastify.llm` is undefined, status is "not_configured".
   - Note: For providers like Ollama, models.list() also works. For OpenAI-compatible APIs, this is a standard endpoint.

4. **Error rate** (new): Call `getRecentErrorRate()` from Task 1.
   - If count > 10 in the 5-minute window, flag as "elevated".
   - Otherwise "normal".

Run checks 2 and 3 in parallel with `Promise.allSettled([webhookCheck(), llmCheck()])` to avoid one slow check blocking the other. Check 1 (database) is synchronous so it runs inline.

**Response shape:**
```json
{
  "status": "ok" | "degraded",
  "timestamp": "ISO string",
  "uptime": 12345.67,
  "checks": {
    "database": "ok" | "error",
    "telegram_webhook": {
      "status": "ok" | "misconfigured" | "error" | "not_configured",
      "url": "...",
      "pending_update_count": 0,
      "last_error_message": null
    },
    "llm": {
      "status": "ok" | "error" | "not_configured"
    },
    "error_rate": {
      "status": "normal" | "elevated",
      "count": 3,
      "window_minutes": 5
    }
  }
}
```

**Overall status logic:** "ok" if database is ok AND all configured services are ok AND error rate is normal. "degraded" otherwise. Return 200 for "ok", 503 for "degraded".

Access `fastify.telegramMessaging` via the existing Fastify decorator (declared in telegram-messaging.ts). Access `fastify.llm` via the existing decorator (declared in conversation.ts). Both are optional (may be undefined if not configured).
  </action>
  <verify>
1. `npx tsc --noEmit` passes
2. `npm run check` passes (linting)
3. Read the file to confirm:
   - All four checks are present (database, telegram_webhook, llm, error_rate)
   - Promise.allSettled is used for parallel webhook + LLM checks
   - Overall status degrades when any configured check fails
   - Response includes all fields from the specified shape
   - Timeouts are applied to external calls (5 second)
  </verify>
  <done>GET /health returns structured JSON with database, telegram_webhook, llm, and error_rate checks. Status is "degraded" when any configured check fails. External checks run in parallel with 5-second timeouts.</done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes
2. `npm run check` passes
3. GET /health returns all four check categories
4. Webhook check uses getWebhookInfo() from Plan 19-01
5. LLM check uses models.list() with timeout
6. Error rate uses sliding window with 5-minute pruning
7. Overall status is "degraded" when any configured check fails
8. Non-configured services show "not_configured" without degrading overall status
</verification>

<success_criteria>
- Health endpoint returns structured status for database, webhook, LLM, and error rate
- Telegram webhook check validates URL matches expected and reports pending_update_count
- LLM check confirms API reachability via models.list()
- Error rate tracks 5xx responses and caught errors in a 5-minute sliding window
- Non-configured services report "not_configured" without affecting overall status
- Configured services that fail degrade overall status to "degraded" (503)
- All external checks have 5-second timeouts to prevent health endpoint from hanging
</success_criteria>

<output>
After completion, create `.planning/phases/19-webhook-server-resilience/19-02-SUMMARY.md`
</output>
