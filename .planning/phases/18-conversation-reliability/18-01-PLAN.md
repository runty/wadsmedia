---
phase: 18-conversation-reliability
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - src/conversation/history.ts
  - src/conversation/history.test.ts
autonomous: true

must_haves:
  truths:
    - "Consecutive orphaned user messages (no assistant response between them) are pruned before LLM sees them -- only the most recent user message in a run is kept"
    - "The sliding window prefers complete user-assistant pairs, avoiding patterns where the LLM sees fragmented conversation flow"
    - "Tool call pairs (assistant with tool_calls + tool results) are never split by pruning"
    - "The pruning and window optimization are pure functions with no side effects on the stored history"
  artifacts:
    - path: "src/conversation/history.ts"
      provides: "pruneOrphanedUserMessages function and improved buildLLMMessages"
      exports: ["pruneOrphanedUserMessages", "buildLLMMessages"]
    - path: "src/conversation/history.test.ts"
      provides: "Unit tests for pruning and sliding window behavior"
      min_lines: 80
  key_links:
    - from: "src/conversation/history.ts:buildLLMMessages"
      to: "src/conversation/history.ts:pruneOrphanedUserMessages"
      via: "buildLLMMessages calls pruneOrphanedUserMessages before windowing"
      pattern: "pruneOrphanedUserMessages"
    - from: "src/conversation/engine.ts"
      to: "src/conversation/history.ts:buildLLMMessages"
      via: "engine calls buildLLMMessages which now includes pruning"
      pattern: "buildLLMMessages"
---

<objective>
Implement and test orphaned user message pruning and sliding window optimization for conversation history.

Purpose: gpt-4o-mini produces confused/repetitive responses when it sees consecutive user messages with no assistant response between them (orphaned messages from errors, rapid sends, or history corruption). Pruning these before the LLM call and improving the sliding window's message selection eliminates the root cause.

Output: Tested pure functions in history.ts that prune orphaned user messages and build an optimized LLM context window.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@src/conversation/history.ts
@src/conversation/types.ts
@src/conversation/engine.ts
</context>

<feature>
  <name>Orphaned User Message Pruning and Sliding Window Optimization</name>
  <files>src/conversation/history.ts, src/conversation/history.test.ts</files>
  <behavior>
    pruneOrphanedUserMessages(history: ChatMessage[]): ChatMessage[]

    Given a chronological array of ChatMessage objects, removes consecutive user messages
    that have no assistant response between them, keeping only the LAST user message in each
    consecutive run. This ensures the LLM never sees "orphaned" user messages that create
    confusing broken conversation flow.

    Rules:
    1. When multiple consecutive user messages appear (no assistant/tool between them), keep only the last one
    2. Tool messages are treated as part of their parent assistant message -- they don't break a "consecutive user" run
    3. Assistant messages (with or without tool_calls) break a consecutive user run
    4. The function operates on a copy -- does NOT mutate the input array
    5. Empty input returns empty output
    6. A single user message is never pruned

    Cases:
    - [user("hi"), user("hello"), assistant("hey")] -> [user("hello"), assistant("hey")]
    - [user("a"), user("b"), user("c"), assistant("ok")] -> [user("c"), assistant("ok")]
    - [user("a"), assistant("b"), user("c"), user("d")] -> [user("a"), assistant("b"), user("d")]
    - [user("a"), assistant+toolCalls, tool, user("b"), user("c")] -> [user("a"), assistant+toolCalls, tool, user("c")]
    - [user("a")] -> [user("a")]
    - [] -> []
    - [assistant("a"), user("b")] -> [assistant("a"), user("b")]

    buildLLMMessages integration:
    - buildLLMMessages must call pruneOrphanedUserMessages on the history BEFORE applying the sliding window
    - This ensures the window budget is not wasted on orphaned messages
    - All existing buildLLMMessages behavior (tool call pair integrity, sanitization) is preserved
  </behavior>
  <implementation>
    1. Create pruneOrphanedUserMessages as an exported pure function in history.ts
    2. Algorithm: iterate forward through history. Track runs of consecutive user messages. When a non-user message (assistant) is encountered, if the preceding run had >1 user messages, keep only the last. Handle end-of-array the same way.
    3. In buildLLMMessages, add `history = pruneOrphanedUserMessages(history)` as the first line after the empty check
    4. Tests use plain ChatMessage objects (no DB needed) -- construct test data inline
  </implementation>
</feature>

<verification>
Run: `npx vitest run src/conversation/history.test.ts`
All tests pass. Verify:
- Consecutive user messages are pruned to keep only the last
- Tool call pairs remain intact
- buildLLMMessages output never contains consecutive user messages (except at array boundaries where no assistant existed)
- Existing sanitizeToolCallSequences behavior is unchanged
</verification>

<success_criteria>
- pruneOrphanedUserMessages correctly prunes all orphaned user message patterns
- buildLLMMessages integrates pruning before windowing
- All tests pass with `npx vitest run`
- TypeScript compiles cleanly with `npx tsc --noEmit`
</success_criteria>

<output>
After completion, create `.planning/phases/18-conversation-reliability/18-01-SUMMARY.md`
</output>
