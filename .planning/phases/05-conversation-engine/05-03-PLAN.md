---
phase: 05-conversation-engine
plan: 03
type: execute
wave: 3
depends_on: ["05-01", "05-02"]
files_modified:
  - src/conversation/confirmation.ts
  - src/conversation/engine.ts
  - src/plugins/conversation.ts
  - src/plugins/webhook.ts
  - src/server.ts
autonomous: true

must_haves:
  truths:
    - "Active user messages are processed by the LLM and a reply is sent via messaging"
    - "Webhook responds immediately with empty TwiML to avoid Twilio timeout"
    - "Conversation history is loaded from DB, sent to LLM, and new messages saved after"
    - "Destructive actions prompt for confirmation and wait for user's next message"
    - "Pending confirmations expire after 5 minutes and are cleared on unrelated messages"
    - "If a user has a pending confirmation, yes/no responses execute or cancel the action"
  artifacts:
    - path: "src/conversation/confirmation.ts"
      provides: "Pending action CRUD and yes/no detection"
      exports: ["savePendingAction", "getPendingAction", "clearPendingAction", "clearExpiredActions", "isConfirmation", "isDenial"]
    - path: "src/conversation/engine.ts"
      provides: "Top-level processConversation orchestrator"
      exports: ["processConversation"]
    - path: "src/plugins/conversation.ts"
      provides: "Fastify plugin decorating instance with LLM client and tool registry"
      exports: ["default"]
    - path: "src/plugins/webhook.ts"
      provides: "Updated webhook handler wiring active users to conversation engine"
    - path: "src/server.ts"
      provides: "Updated server with conversation plugin registration"
  key_links:
    - from: "src/plugins/webhook.ts"
      to: "src/conversation/engine.ts"
      via: "processConversation() call for active users"
      pattern: "processConversation"
    - from: "src/conversation/engine.ts"
      to: "src/conversation/tool-loop.ts"
      via: "toolCallLoop() for LLM processing"
      pattern: "toolCallLoop"
    - from: "src/conversation/engine.ts"
      to: "src/conversation/history.ts"
      via: "getHistory/saveMessage for persistence"
      pattern: "getHistory|saveMessage"
    - from: "src/conversation/engine.ts"
      to: "src/conversation/confirmation.ts"
      via: "pending action check before LLM call"
      pattern: "getPendingAction"
    - from: "src/plugins/conversation.ts"
      to: "src/conversation/llm.ts"
      via: "createLLMClient for OpenAI client initialization"
      pattern: "createLLMClient"
    - from: "src/server.ts"
      to: "src/plugins/conversation.ts"
      via: "plugin registration"
      pattern: "conversationPlugin"
---

<objective>
Wire the conversation engine end-to-end: confirmation tier system, top-level conversation orchestrator, Fastify plugin, and webhook integration replacing the placeholder response for active users.

Purpose: This plan connects all the pieces from plans 01 and 02 into a working system. After this plan, active users who text the app will receive LLM-powered responses with tool calling capability.
Output: Confirmation module, conversation engine orchestrator, Fastify plugin, updated webhook handler.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-conversation-engine/05-RESEARCH.md
@.planning/phases/05-conversation-engine/05-01-SUMMARY.md
@.planning/phases/05-conversation-engine/05-02-SUMMARY.md
@src/plugins/webhook.ts
@src/server.ts
@src/conversation/types.ts
@src/conversation/llm.ts
@src/conversation/history.ts
@src/conversation/tools.ts
@src/conversation/tool-loop.ts
@src/conversation/system-prompt.ts
@src/config.ts
@src/messaging/types.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Confirmation tier and conversation engine orchestrator</name>
  <files>src/conversation/confirmation.ts, src/conversation/engine.ts</files>
  <action>
**1. Create src/conversation/confirmation.ts** -- Pending action management:

Pure functions with db parameter (consistent with user.service.ts and history.ts patterns).

`savePendingAction(db, action: PendingAction)`: Upsert into pendingActions table (one per user via unique constraint on userId). If a pending action already exists for this user, replace it.

`getPendingAction(db, userId)`: Get the pending action for a user. Returns null if none exists or if the existing one has expired (expiresAt < now).

`clearPendingAction(db, userId)`: Delete the pending action for a user.

`clearExpiredActions(db)`: Delete all pending actions where expiresAt < now. Called opportunistically, not on a timer.

`isConfirmation(message: string): boolean`: Check if a message is an affirmative confirmation. Normalize to lowercase and trim. Match against: "yes", "y", "confirm", "do it", "go ahead", "ok", "sure", "yeah", "yep".

`isDenial(message: string): boolean`: Check if a message is a denial. Match against: "no", "n", "cancel", "stop", "nevermind", "nah", "nope".

**2. Create src/conversation/engine.ts** -- Top-level orchestrator:

`processConversation(params)` -- Async function. This is the main entry point called from the webhook handler.

Params:
```
{
  userId: number,
  userPhone: string,
  displayName: string | null,
  messageBody: string,
  db: DB,
  llmClient: OpenAI,
  registry: ToolRegistry,
  sonarr?: SonarrClient,
  radarr?: RadarrClient,
  messaging: MessagingProvider,
  config: AppConfig,
  log: FastifyBaseLogger
}
```

Algorithm:
1. **Clear expired pending actions** (opportunistic cleanup): `clearExpiredActions(db)`
2. **Check for pending confirmation**: `getPendingAction(db, userId)`
   - If pending action exists:
     a. If `isConfirmation(messageBody)`:
        - Look up the tool in the registry
        - Execute it with the stored arguments and context
        - Clear the pending action
        - Save user message ("yes") and assistant message (execution result) to history
        - Send reply via messaging.send(): "Done! {brief result description}" or format based on tool result
        - Return
     b. If `isDenial(messageBody)`:
        - Clear the pending action
        - Save user message and assistant message ("Cancelled.") to history
        - Send reply: "OK, cancelled."
        - Return
     c. Otherwise (unrelated message):
        - Clear the pending action (stale, user moved on)
        - Fall through to normal LLM processing below
3. **Save the user message** to history: `saveMessage(db, { userId, role: "user", content: messageBody })`
4. **Load conversation history**: `getHistory(db, userId)`
5. **Build LLM messages**: `buildLLMMessages(buildSystemPrompt(displayName), history, 20)`
6. **Run tool call loop**: `toolCallLoop({ client: llmClient, model: config.LLM_MODEL, messages: llmMessages, registry, context: { sonarr, radarr, userId }, log })`
7. **Handle result**:
   - Save all new messages produced by the loop to history (assistant messages with tool_calls, tool result messages, and the final assistant text reply). Iterate through `result.messagesConsumed` and save any messages that weren't already in the history (i.e., messages after the ones loaded in step 4).
   - If `result.pendingConfirmation`: save the pending action via `savePendingAction(db, result.pendingConfirmation)`
   - Send the reply to the user: `messaging.send({ to: userPhone, body: result.reply, from: config.TWILIO_PHONE_NUMBER })`
8. **Error handling**: Wrap the entire function in try/catch. On error, log it and send a fallback message: "Sorry, something went wrong. Please try again."

Type the DB parameter as `BetterSQLite3Database<typeof schema>` (same pattern as onboarding.ts).
  </action>
  <verify>
1. `npx tsc --noEmit` passes
2. `npm run check` passes
3. Review processConversation flow covers: pending confirmation handling (yes/no/unrelated), normal LLM flow, history persistence, error handling
  </verify>
  <done>
- Confirmation module handles pending action CRUD, expiration, and yes/no detection
- processConversation orchestrates the full conversation flow: confirmation check -> history load -> LLM call -> result persistence -> reply send
- Error handling sends fallback message to user
- All new messages from the LLM loop are persisted to history
  </done>
</task>

<task type="auto">
  <name>Task 2: Fastify plugin and webhook integration</name>
  <files>src/plugins/conversation.ts, src/plugins/webhook.ts, src/server.ts</files>
  <action>
**1. Create src/plugins/conversation.ts** -- Fastify plugin:

Follow the existing plugin pattern (fp wrapper, dependencies, decorate).

```typescript
declare module "fastify" {
  interface FastifyInstance {
    llm: OpenAI;
    toolRegistry: ToolRegistry;
  }
}
```

Plugin logic:
- Validate that LLM_API_KEY or LLM_BASE_URL is configured (at least one must be present -- OpenAI needs the key, Ollama needs the URL). If neither is configured, log a warning and skip registration (same graceful degradation pattern as sonarr/radarr plugins).
- Create the OpenAI client: `createLLMClient(fastify.config)`
- Create the tool registry: `createToolRegistry()` (this registers the check_status tool from plan 02)
- Decorate: `fastify.decorate("llm", client)` and `fastify.decorate("toolRegistry", registry)`
- Log: `fastify.log.info({ model: fastify.config.LLM_MODEL }, "Conversation engine initialized")`
- Dependencies: `["database"]` (messages table needs DB, consistent with other plugins)

**2. Update src/plugins/webhook.ts** -- Replace placeholder with conversation engine:

Replace the active user block (currently lines 47-50 with the placeholder "Conversation features coming soon!" response) with:

```typescript
if (user.status === "active") {
  // Respond immediately to Twilio to avoid 15-second timeout
  reply.type("text/xml").send(fastify.messaging.formatEmptyReply());

  // Process conversation asynchronously (fire-and-forget from Twilio's perspective)
  // Only if conversation engine is configured
  if (fastify.llm && fastify.toolRegistry) {
    processConversation({
      userId: user.id,
      userPhone: user.phone,
      displayName: user.displayName,
      messageBody: message.body,
      db: fastify.db,
      llmClient: fastify.llm,
      registry: fastify.toolRegistry,
      sonarr: fastify.sonarr,
      radarr: fastify.radarr,
      messaging: fastify.messaging,
      config: fastify.config,
      log: request.log,
    }).catch((err) => {
      request.log.error({ err }, "Conversation processing failed");
      fastify.messaging.send({
        to: user.phone,
        body: "Sorry, something went wrong. Please try again.",
        from: fastify.config.TWILIO_PHONE_NUMBER,
      }).catch((sendErr) => {
        request.log.error({ err: sendErr }, "Failed to send error message");
      });
    });
  } else {
    // LLM not configured -- send a helpful message
    fastify.messaging.send({
      to: user.phone,
      body: "The conversation engine is not configured yet. Please set LLM_API_KEY and LLM_MODEL environment variables.",
      from: fastify.config.TWILIO_PHONE_NUMBER,
    }).catch((sendErr) => {
      request.log.error({ err: sendErr }, "Failed to send config message");
    });
  }

  return;
}
```

Add import for processConversation at the top of webhook.ts.

Update the webhook plugin dependencies to include "conversation" (optional -- don't make it required since the conversation plugin may not register if LLM is unconfigured). Actually, since the webhook already checks `fastify.llm` existence, and the conversation plugin uses graceful skip, the dependency is NOT needed. Keep dependencies as-is: `["messaging", "user-resolver"]`.

**3. Update src/server.ts** -- Register conversation plugin:

Add import: `import conversationPlugin from "./plugins/conversation.js";`

Register AFTER sonarr/radarr plugins but BEFORE webhook plugin:
```typescript
await fastify.register(conversationPlugin);
```

The registration order should be:
1. databasePlugin
2. healthPlugin
3. sonarrPlugin
4. radarrPlugin
5. conversationPlugin  <-- NEW
6. formbody
7. messagingPlugin
8. userResolverPlugin
9. webhookPlugin

This ensures the LLM client and tool registry are available when the webhook handler fires.
  </action>
  <verify>
1. `npx tsc --noEmit` passes
2. `npm run check` passes
3. `npm run build` succeeds
4. Review that server.ts registers conversation plugin in correct order
5. Review that webhook.ts responds with empty TwiML immediately, then processes async
6. Review that webhook.ts gracefully handles missing LLM configuration
  </verify>
  <done>
- Conversation Fastify plugin creates LLM client and tool registry, decorates fastify instance
- Plugin gracefully skips if LLM is not configured (same pattern as sonarr/radarr)
- Webhook handler responds immediately with empty TwiML for active users (avoids Twilio timeout)
- Conversation processing runs asynchronously after webhook response
- Active users with configured LLM get full conversation engine processing
- Active users without LLM config get a helpful configuration message
- Server registers conversation plugin in correct dependency order
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` -- all files compile
2. `npm run check` -- Biome passes
3. `npm run build` -- production build succeeds
4. Webhook responds immediately with empty TwiML (no Twilio timeout risk)
5. Conversation processing is fire-and-forget from webhook perspective
6. Graceful degradation when LLM is not configured
7. Pending confirmations expire after 5 minutes
8. Yes/no confirmation responses execute or cancel pending destructive actions
</verification>

<success_criteria>
- Active user messages are routed through the full LLM conversation pipeline
- Twilio webhook responds immediately (empty TwiML) to avoid 15-second timeout
- LLM replies are sent as separate outbound messages via messaging.send()
- Conversation history is persisted per user and loaded for each conversation turn
- Destructive tool calls prompt for confirmation; next yes/no response executes or cancels
- Pending actions expire after 5 minutes; unrelated messages clear stale pending actions
- App starts and functions normally when LLM is not configured (graceful degradation)
- The placeholder "Conversation features coming soon!" response is replaced with actual LLM processing
</success_criteria>

<output>
After completion, create `.planning/phases/05-conversation-engine/05-03-SUMMARY.md`
</output>
