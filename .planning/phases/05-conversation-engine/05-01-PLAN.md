---
phase: 05-conversation-engine
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/db/schema.ts
  - src/conversation/types.ts
  - src/conversation/llm.ts
  - src/conversation/history.ts
  - src/conversation/system-prompt.ts
autonomous: true
user_setup:
  - service: openai-compatible-llm
    why: "LLM inference for conversation engine"
    env_vars:
      - name: LLM_API_KEY
        source: "OpenAI Dashboard -> API keys, or provider equivalent (Ollama uses 'not-needed')"
      - name: LLM_BASE_URL
        source: "Provider base URL (e.g., https://api.openai.com/v1, http://localhost:11434/v1 for Ollama)"
      - name: LLM_MODEL
        source: "Model identifier (e.g., gpt-4o, llama3.1)"

must_haves:
  truths:
    - "Conversation messages are persisted per user in SQLite"
    - "Stored messages survive server restarts"
    - "LLM client can be constructed with configurable baseURL and apiKey"
    - "History retrieval returns messages in chronological order with sliding window"
    - "Tool call message pairs (assistant tool_calls + tool results) are never split by the sliding window"
  artifacts:
    - path: "src/db/schema.ts"
      provides: "messages and pendingActions table definitions"
      contains: "messages"
    - path: "src/conversation/types.ts"
      provides: "Shared types for conversation engine"
      exports: ["ChatMessage", "ToolDefinition", "ToolContext", "PendingAction", "ToolCallLoopResult"]
    - path: "src/conversation/llm.ts"
      provides: "OpenAI SDK wrapper with configurable provider"
      exports: ["createLLMClient"]
    - path: "src/conversation/history.ts"
      provides: "Conversation history CRUD and sliding window"
      exports: ["saveMessage", "getHistory", "buildLLMMessages"]
    - path: "src/conversation/system-prompt.ts"
      provides: "System prompt template"
      exports: ["SYSTEM_PROMPT"]
  key_links:
    - from: "src/conversation/history.ts"
      to: "src/db/schema.ts"
      via: "drizzle query on messages table"
      pattern: "messages"
    - from: "src/conversation/history.ts"
      to: "src/conversation/types.ts"
      via: "ChatMessage type import"
      pattern: "ChatMessage"
---

<objective>
Create the data layer and LLM client for the conversation engine: database schema for message persistence, typed conversation history with sliding window, configurable OpenAI-compatible LLM client, and system prompt.

Purpose: Establishes the foundation that all other conversation engine plans build on -- message storage, LLM connectivity, and history management with proper tool call boundary handling.
Output: Database tables (messages, pending_actions), LLM client factory, history module with sliding window, shared types, system prompt.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-conversation-engine/05-RESEARCH.md
@src/db/schema.ts
@src/db/index.ts
@src/config.ts
@src/users/user.types.ts
@src/users/onboarding.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Database schema and conversation types</name>
  <files>src/db/schema.ts, src/conversation/types.ts</files>
  <action>
**1. Extend src/db/schema.ts** with two new tables (add below the existing `users` table):

`messages` table:
- `id`: integer primary key, autoIncrement
- `userId`: integer, notNull, references users.id
- `role`: text with enum ["user", "assistant", "tool", "system"], notNull
- `content`: text (nullable -- assistant messages with only tool_calls have null content)
- `toolCalls`: text column named "tool_calls" (nullable -- JSON stringified tool_calls array, assistant messages only)
- `toolCallId`: text column named "tool_call_id" (nullable -- for role:"tool" messages, links to specific tool call)
- `name`: text (nullable -- function name for tool messages)
- `createdAt`: integer("created_at", { mode: "timestamp" }), notNull, $defaultFn(() => new Date())

`pendingActions` table:
- `id`: integer primary key, autoIncrement
- `userId`: integer, notNull, references users.id, unique (one pending action per user)
- `functionName`: text("function_name"), notNull
- `arguments`: text, notNull (JSON stringified)
- `promptText`: text("prompt_text"), notNull
- `createdAt`: integer("created_at", { mode: "timestamp" }), notNull, $defaultFn(() => new Date())
- `expiresAt`: integer("expires_at", { mode: "timestamp" }), notNull

**2. Create src/conversation/types.ts** with shared types:

```typescript
import type { ChatCompletionMessageParam, ChatCompletionTool } from "openai/resources/chat/completions";

// Re-export OpenAI message type for convenience
export type { ChatCompletionMessageParam, ChatCompletionTool };

// Stored message shape (maps to messages table row)
export interface ChatMessage {
  id: number;
  userId: number;
  role: "user" | "assistant" | "tool" | "system";
  content: string | null;
  toolCalls: string | null;  // JSON stringified
  toolCallId: string | null;
  name: string | null;
  createdAt: Date;
}

// Confirmation tier classification
export type ConfirmationTier = "safe" | "destructive";

// Tool definition with metadata for registry
export interface ToolDefinition {
  definition: ChatCompletionTool;
  tier: ConfirmationTier;
  paramSchema: unknown;  // Zod schema for argument validation
  execute: (args: unknown, context: ToolContext) => Promise<unknown>;
}

// Context passed to tool executors
export interface ToolContext {
  sonarr?: import("../media/sonarr/sonarr.client.js").SonarrClient;
  radarr?: import("../media/radarr/radarr.client.js").RadarrClient;
  userId: number;
}

// Result from the tool call loop
export interface ToolCallLoopResult {
  reply: string;
  pendingConfirmation?: PendingAction;
  messagesConsumed: ChatCompletionMessageParam[];
}

// Pending destructive action awaiting user confirmation
export interface PendingAction {
  userId: number;
  functionName: string;
  arguments: string;  // JSON
  promptText: string;
  expiresAt: Date;
}
```

After writing both files, install the openai package: `npm install openai`

Generate a new Drizzle migration: `npm run db:generate`

Verify the migration SQL file was created in the drizzle/ directory.
  </action>
  <verify>
1. `npx tsc --noEmit` passes (types resolve correctly)
2. `npm run db:generate` creates a migration file in drizzle/ containing CREATE TABLE for messages and pending_actions
3. `npm run check` (Biome) passes
  </verify>
  <done>
- messages and pendingActions tables defined in schema.ts with correct column types and foreign key references
- src/conversation/types.ts exports ChatMessage, ToolDefinition, ToolContext, ToolCallLoopResult, PendingAction, ConfirmationTier
- openai package installed in dependencies
- Drizzle migration generated
  </done>
</task>

<task type="auto">
  <name>Task 2: LLM client, conversation history, and system prompt</name>
  <files>src/conversation/llm.ts, src/conversation/history.ts, src/conversation/system-prompt.ts</files>
  <action>
**1. Create src/conversation/llm.ts** -- OpenAI SDK client factory:

```typescript
import OpenAI from "openai";
import type { AppConfig } from "../config.js";

export function createLLMClient(config: AppConfig): OpenAI {
  return new OpenAI({
    apiKey: config.LLM_API_KEY ?? "not-needed",  // Ollama ignores apiKey
    ...(config.LLM_BASE_URL ? { baseURL: config.LLM_BASE_URL } : {}),
  });
}
```

Simple factory, no class wrapper. The OpenAI SDK handles retries, types, and error classes.

**2. Create src/conversation/history.ts** -- Conversation history CRUD with sliding window:

This module uses pure functions with db parameter (consistent with user.service.ts pattern).

Functions to implement:

`saveMessage(db, params)`: Insert a message row into the messages table. Params: `{ userId, role, content, toolCalls?, toolCallId?, name? }`. Returns the inserted row.

`getHistory(db, userId, limit?)`: Retrieve the last N messages for a user ordered by createdAt ASC, id ASC. Default limit = 50 (raw retrieval limit, not the LLM window).

`buildLLMMessages(systemPrompt, history, maxMessages?)`: Convert stored ChatMessage rows into OpenAI ChatCompletionMessageParam array with sliding window. Default maxMessages = 20.

CRITICAL sliding window logic: The window must never split tool call pairs. Algorithm:
1. Start from the end of history, work backward counting messages.
2. When encountering a `role: "tool"` message, find its parent assistant message (the one with matching tool_calls containing that toolCallId). Include the full group.
3. When encountering an assistant message with tool_calls, include all its corresponding tool result messages.
4. Always start the window at either a "user" message or the very beginning of a complete tool call sequence (assistant + all tool results).
5. Prepend the system prompt as the first message.

Converting stored messages to ChatCompletionMessageParam:
- For assistant messages with toolCalls: parse the JSON string back into the tool_calls array.
- For tool messages: include tool_call_id and content.
- For user/system messages: just role and content.

`clearHistory(db, userId)`: Delete all messages for a user (utility for future use).

**3. Create src/conversation/system-prompt.ts**:

Export a `SYSTEM_PROMPT` constant string. Use the template from research:

```
You are a helpful media management assistant. You help users search for, add, and manage movies and TV shows using Sonarr and Radarr.

Available capabilities:
- Search for movies and TV shows by title
- Check what's in the user's library
- Add movies or shows to the download list
- Remove media (requires user confirmation)
- Check download queue status
- View upcoming episodes and releases

Guidelines:
- Be concise. Users are texting via SMS, so keep responses short.
- When search returns multiple results, present the top 3-5 with enough detail to distinguish them (title, year, overview snippet).
- For add operations, use sensible defaults (first root folder, first quality profile) unless the user specifies otherwise.
- Never execute remove/delete operations without explicit user confirmation.
- If a tool call fails, explain the error simply and suggest next steps.
- Refer to the user by name when available.
```

Also export a `buildSystemPrompt(displayName?: string | null)` function that appends `\n\nThe user's name is {displayName}.` when displayName is a non-empty string, so the LLM can personalize responses.
  </action>
  <verify>
1. `npx tsc --noEmit` passes
2. `npm run check` passes
3. Verify buildLLMMessages correctly handles the sliding window by inspecting the logic for tool call boundary preservation
  </verify>
  <done>
- createLLMClient accepts AppConfig and returns an OpenAI client with configurable baseURL
- saveMessage persists messages to the messages table
- getHistory retrieves messages for a user in chronological order
- buildLLMMessages applies sliding window that never splits tool call pairs, prepends system prompt
- clearHistory deletes all messages for a user
- SYSTEM_PROMPT and buildSystemPrompt exported with personalization support
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` -- all new files compile without errors
2. `npm run check` -- Biome lint/format passes
3. Drizzle migration exists for messages and pending_actions tables
4. `openai` appears in package.json dependencies
5. All exports resolve: types.ts, llm.ts, history.ts, system-prompt.ts
</verification>

<success_criteria>
- Database schema extended with messages and pendingActions tables
- Drizzle migration generated and committed
- OpenAI SDK installed and LLM client factory created
- Conversation history module handles CRUD and sliding window with tool call boundary preservation
- System prompt template ready with personalization
- All TypeScript types compile cleanly
</success_criteria>

<output>
After completion, create `.planning/phases/05-conversation-engine/05-01-SUMMARY.md`
</output>
